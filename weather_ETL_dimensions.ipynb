{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import IntegerType\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0ac67614-018d-4c69-9a9e-22fe2cf699b2","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Creating dim_date\ndef create_dim_date(start_date, end_date):\n    # Creating sequence to create a list of date from start_date to end_date at interval of 1 day\n    date_list = spark.sql(f\"select sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day ) as dates \").collect()[0].dates\n    \n    # Making dataframe from the list\n    date_df = spark.createDataFrame([(d,) for d in date_list], [\"date_alt_key\"])\n    \n    # Adding necessary fields to the dataframe\n    date_df = date_df.withColumn('date_id', date_format(col(\"date_alt_key\"), \"yyyyMMdd\"))\\\n                     .withColumn('year', year(\"date_alt_key\")) \\\n                     .withColumn('quarter', quarter(\"date_alt_key\")) \\\n                     .withColumn('month', month(\"date_alt_key\")) \\\n                     .withColumn('day', dayofmonth(\"date_alt_key\")) \\\n                     .withColumn('day_of_week', date_format(col(\"date_alt_key\"), 'EEEE'))\n    \n    # Managing the order\n    date_df = date_df.select('date_id', 'date_alt_key', 'year', 'quarter', 'month', 'day', 'day_of_week')\n    \n    # Writing to dim_date\n    date_df.write.format('delta').mode('overwrite').saveAsTable('dim_date')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"75424a72-519a-4407-bbe3-085ab028f786","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Creating dim_time\ndef create_dim_time(start_time, end_time, interval):\n    # Creating sequence to create a list of time from start_time to end_time at interval of 1 hour\n    time_list = spark.sql(f\"select sequence(to_timestamp('{start_time}', 'HH:mm:ss'), to_timestamp('{end_time}','HH:mm:ss'), interval '{interval}')\\\n                    AS time_alt\").collect()[0].time_alt\n    \n    # Creaing dataframe using the list\n    time_df = spark.createDataFrame([(t,) for t in time_list], ['time_alt'])\n    \n    # Adding necessary fields to the dataframe\n    time_df = time_df.withColumn('time_alt_key', date_format(col('time_alt'), 'HH:mm:ss'))\n    \n    windowSpec = Window.orderBy('time_alt_key')\n    time_df = time_df.withColumn('time_id', row_number().over(windowSpec).cast(IntegerType()))\n    time_df = time_df.select('time_id', 'time_alt_key')\n    \n    # Writing to dim_time\n    time_df.write.format('delta').mode('overwrite').saveAsTable('dim_time')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7d8352af-02c9-440d-be6e-e5fff4c519e1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Creating the dim_city\ndef create_dim_city():\n    city_df = spark.read.format('delta').load('dbfs:/FileStore/shared_uploads/export_deltatable')\n    city_df = city_df.select('id','name','lat','lon').limit(5)\n    \n    city_df.write.format('delta').mode('overwrite').saveAsTable('dim_city')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"514f9248-da22-4cb2-8089-4473053551f0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"weather_ETL_dimensions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1604498312634307,"dataframes":["_sqldf"]}},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
