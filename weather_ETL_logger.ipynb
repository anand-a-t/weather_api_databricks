{"cells":[{"cell_type":"code","source":["%run \"./weather_ETL_schemas\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a833a4f9-842a-474c-aae6-7af200d003d9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col, from_unixtime, lit\nfrom datetime import datetime\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType, ArrayType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d841d691-9efa-4289-b36a-461682748a76","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Using decorator for implementing the log table\ndef update_log(func):\n    def wrapper_function(*args):\n        \n        # Extracting the necessary fields\n        id = str(uuid.uuid4())\n        load_type = args[0]\n        table_name = args[1]\n        comments = args[2]\n        process_start_time = datetime.now()\n        status = 'EXTRACTING'\n        \n        load_run_id = id\n        created_by = 'Ananda Thakur'\n        created_on = process_start_time\n        \n        # Creating log record for start of process\n        log_data = [(id, load_type, table_name, process_start_time, None, status, comments, None, None, created_on, created_by)]\n        log_df = spark.createDataFrame(log_data, schema=get_log_schema())\n        log_df.write.format('delta').mode('append').saveAsTable('log_table_new')\n        \n        process_start_time = datetime.now()\n        created_on = process_start_time\n        \n        # Creating necessary columns and saving to table and returning ERROR if the process fails\n        try:\n            df, start_dt, end_dt = func(*args[3:])\n\n            df = df.withColumn('load_run_id', lit(load_run_id))\n            df = df.withColumn('created_on', lit(created_on))\n            df = df.withColumn('created_by', lit(created_by))\n\n            df.write.format('delta').mode('append').saveAsTable(table_name)\n\n            status = 'COMPLETED'\n        except:\n            status = 'ERROR'\n        \n        process_end_time = datetime.now()\n        \n        # Creating log record for start of process\n        log_data = [(id, load_type, table_name, process_start_time, process_end_time, status, comments, start_dt, end_dt, created_on, created_by)]\n        log_df = spark.createDataFrame(log_data, schema=get_log_schema())\n        log_df.write.format('delta').mode('append').saveAsTable('log_table_new')\n        \n        return df\n    return wrapper_function"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6a00d04c-8088-4e13-9712-ee0e35dd6271","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e18030d9-5ecc-49cd-a5ae-45786e844ac4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"weather_ETL_logger","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
